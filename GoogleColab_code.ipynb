!pip install --upgrade google-cloud-discoveryengine google-cloud-aiplatform -q
!gcloud services enable aiplatform.googleapis.com --project "your_Google_Cloud_project"

import warnings
warnings.filterwarnings("ignore")
from google.colab import auth
import vertexai
from vertexai.generative_models import GenerativeModel
from google.cloud import discoveryengine_v1 as discoveryengine

# 1. Authentication & Setup
auth.authenticate_user()

PROJECT_ID = "your_Google_Cloud_project"
DATA_STORE_ID = "your_ID_data_store"
LOCATION = "global"

vertexai.init(project=PROJECT_ID, location="us-central1")

# --- BACKEND: TURBO RAG ENGINE ---
def turbo_rag_engine(user_query):
    client = discoveryengine.SearchServiceClient()
    serving_config = client.serving_config_path(
        project=PROJECT_ID,
        location=LOCATION,
        data_store=DATA_STORE_ID,
        serving_config="default_config",
    )

    # Configuration: Aggressive retrieval (Snippets + Extractive Segments)
    req = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=user_query,
        page_size=10,
        content_search_spec=discoveryengine.SearchRequest.ContentSearchSpec(
            snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(
                return_snippet=True
            ),
            extractive_content_spec=discoveryengine.SearchRequest.ContentSearchSpec.ExtractiveContentSpec(
                max_extractive_answer_count=1,
                max_extractive_segment_count=1
            )
        )
    )

    try:
        response = client.search(req)
    except Exception as e:
        return f"‚ùå Technical Error: {e}"

    context_data = ""
    if response.results:
        for result in response.results:
            data = result.document.derived_struct_data

            # Greedy Extraction Strategy: Accumulate ALL available text
            chunk = ""

            # 1. Extractive Segments (High precision)
            if "extractive_segments" in data:
                for seg in data["extractive_segments"]:
                    chunk += seg.get("content", "") + "\n"

            # 2. Snippets (Search summaries)
            if "snippets" in data:
                for snip in data["snippets"]:
                    chunk += snip.get("snippet", "") + "\n"

            # 3. Extractive Answers (Direct answers)
            if "extractive_answers" in data:
                 for ans in data["extractive_answers"]:
                     chunk += ans.get("content", "") + "\n"

            if chunk:
                context_data += f"--- FRAGMENT ---\n{chunk}\n"

    if not context_data:
        return "‚ö†Ô∏è The search engine returned no readable text. Try using English keywords."

    # Model: Gemini 2.0 Flash
    model = GenerativeModel("gemini-2.0-flash-001")

    # Prompt Engineering
    prompt = f"""
    Role: Senior Financial Analyst & Data Scientist.
    Task: You have access to retrieved fragments from corporate shareholder letters.

    RETRIEVED CONTEXT:
    {context_data[:50000]}

    USER QUESTION: "{user_query}"

    INSTRUCTIONS:
    1. Answer strictly based on the provided CONTEXT.
    2. Identify specific figures (e.g., $619 million), risks, and strategic mistakes.
    3. Explicitly cite the company name (Netflix, Tesla, Berkshire).
    4. Maintain a professional, technical tone.
    5. If the answer is not in the context, state: "Information not found in the documents."
    """

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini API Error: {e}"

# --- FRONTEND: INTERACTIVE CHAT ---
print("‚úÖ SYSTEM READY. Type 'exit' to quit.")
print("=======================================================")

while True:
    user_input = input("\nAsk the AI Analyst: ")

    if user_input.lower() in ["exit", "quit", "bye"]:
        print("üëã Session closed.")
        break

    print(f"üîç Searching deep in the cloud...")
    result = turbo_rag_engine(user_input)
    print("\n" + result)
    print("-" * 50)
